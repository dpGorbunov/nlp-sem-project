\documentclass{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtext}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}

\title{Mahalanobis OOD Detection for AI-Generated Text Classification}
\author{Dmitry Gorbunov\\ITMO University, AI Talent Hub}
\date{2025}

\begin{document}
\maketitle

\begin{abstract}
We reproduce the AINL-Eval 2025 winning solution (sastsy, 91.22\%) for detecting AI-generated scientific abstracts in Russian. The key challenge is identifying texts from unknown AI models not seen during training. We apply Mahalanobis distance-based OOD detection to Qwen2.5-7B with Dual-Head architecture, improving accuracy by +10.4\% over softmax confidence (from 79.53\% to 89.97\%) with 76.25\% unknown class recall. Mahalanobis also boosts lightweight ruBERT-tiny2 (29M params) to 85.25\% -- only 4.7\% below Qwen, but 20x faster (15ms CPU) and 127x smaller. Code: \url{https://github.com/dpGorbunov/nlp-sem-project}.
\end{abstract}

\section{Introduction}

As LLMs become increasingly capable, distinguishing human-written scientific texts from AI-generated ones grows harder. A student can now generate a plausible abstract in seconds, and reviewers cannot reliably tell the difference. This threatens the integrity of scientific publishing.

The AINL-Eval 2025 shared task~\cite{ainleval2025} tackles this challenge for Russian scientific abstracts. The task is not just binary (human vs AI) -- it requires identifying \textit{which} AI model generated the text, including an "unknown" class for models not seen during training. This makes the problem significantly harder: the system must generalize to new AI models.

The winning solution by team sastsy achieved 91.22\% accuracy using GigaCheck~\cite{gigacheck2024} with a Dual-Head modification. We reproduce and extend this approach:

\begin{enumerate}
    \item \textbf{Qwen2.5-7B}: stronger backbone than Mistral-7B on benchmarks
    \item \textbf{Mahalanobis OOD Detection}: distance-based method for unknown class detection (+10.4\% over baseline)
    \item \textbf{Knowledge Distillation}: distillation to ruBERT-tiny for CPU inference
\end{enumerate}

\subsection{Team}
\textbf{Dmitry Gorbunov} -- model architecture design, experiments, report writing.

\section{Related Work}
\label{sec:related}

\textbf{GigaCheck}~\cite{gigacheck2024} established a strong baseline for LLM-generated content detection using Mistral-7B fine-tuned with LoRA~\cite{hu2022lora}. LoRA enables efficient adaptation by learning low-rank updates: $W' = W + \frac{\alpha}{r} \cdot BA$, where only matrices $B$ and $A$ are trained. GigaCheck uses EOS token pooling and a single classification head.

\textbf{The sastsy solution}~\cite{ainleval2025} improved GigaCheck by splitting classification into two heads: binary (human vs AI) and multiclass (which AI model). This separation helps because detecting AI is easier than identifying the specific model -- the binary head achieves 95\%+ accuracy while multiclass is harder.

\textbf{Knowledge Distillation} allows compressing large models into smaller ones. DisRanker~\cite{disranker2024} showed that LLM knowledge can be distilled to BERT with 10x speedup and minimal quality loss, motivating our experiments with ruBERT-tiny2.

\textbf{Mahalanobis distance}~\cite{mahalanobis2018} measures how far a sample is from a class distribution, accounting for correlations:
$$D_M(x, c) = \sqrt{(x - \mu_c)^T \Sigma^{-1} (x - \mu_c)}$$
Unlike softmax confidence, which only considers the model's output logits, Mahalanobis operates in embedding space and can detect samples that are far from all known classes -- exactly what we need for the "unknown" AI model problem.

\section{Model Description}

\subsection{Architecture Overview}
We follow the sastsy architecture but replace Mistral-7B with Qwen2.5-7B. Why Qwen? According to the Qwen2 Technical Report~\cite{qwen2024}, it outperforms Mistral on standard benchmarks (Tab.~\ref{tab:qwen_mistral}), suggesting better text understanding.

The model has four components:
\begin{enumerate}
    \item \textbf{Backbone}: Qwen2.5-7B fine-tuned with LoRA (r=8, alpha=16) -- we only train 0.04\% of parameters
    \item \textbf{Pooling}: EOS token embedding (the last token captures the full sequence context)
    \item \textbf{Shared Layer}: Linear + tanh + dropout (transforms embeddings before classification)
    \item \textbf{Dual-Head}: Two classification heads working together
\end{enumerate}

\subsection{Dual-Head Architecture}
The key insight from sastsy: separate easy and hard tasks.

\textbf{Binary Head} answers: "Is this AI-generated?" This is relatively easy -- AI texts have subtle but consistent patterns.

\textbf{Multiclass Head} answers: "Which AI model?" This is harder -- different LLMs produce similar outputs.

During training, both heads are optimized jointly:
$$\mathcal{L} = \mathcal{L}_{CE}^{bin} + \mathcal{L}_{CE}^{multi}$$
The multiclass loss ignores human samples (they have no AI model label).

During inference: if binary predicts "human" â†’ output human. Otherwise, use multiclass prediction. The "unknown" class is not predicted directly -- it is detected via Mahalanobis distance on embeddings.

\begin{table}[tbh!]
\centering
\begin{tabular}{|l|ccc|}
\hline
Benchmark & Qwen2-7B & Mistral-7B & $\Delta$ \\
\hline
MMLU & \textbf{70.3} & 64.2 & +6.1 \\
HumanEval & \textbf{51.2} & 29.3 & +21.9 \\
GSM8K & \textbf{79.9} & 52.2 & +27.7 \\
\hline
\end{tabular}
\caption{Qwen2-7B vs Mistral-7B benchmark comparison.}
\label{tab:qwen_mistral}
\end{table}

\section{Dataset}

The AINL-Eval 2025 dataset~\cite{ainleval2025} contains Russian scientific abstracts from four sources: human-written, GPT-4-Turbo, Llama-3.3-70B, and Gemma-2-27B (Tab.~\ref{tab:dataset}).

\begin{table}[tbh!]
\centering
\begin{tabular}{|l|ccc|}
\hline
 & Train & Dev & Test \\
\hline
Samples & 35,158 & 10,979 & 6,169 \\
Classes & 4 & 5 & 5 \\
\hline
\end{tabular}
\caption{AINL-Eval 2025 dataset statistics.}
\label{tab:dataset}
\end{table}

The critical twist: dev and test sets include a fifth class -- "unknown" -- generated by models \textit{not present in training} (GigaChat-Lite in dev, DeepSeek-V3 in test). A classifier trained on four classes has never seen these models and must somehow recognize "this looks like AI, but not any AI I know."

This is the core OOD detection challenge. Standard softmax confidence fails here: the model confidently misclassifies unknown samples as one of the known AI models. Mahalanobis distance solves this by measuring distance in embedding space -- unknown samples are far from all class centroids.

\textbf{Interesting observation}: Human texts are longer (126 words on average) and contain 10x more digits than AI-generated ones~\cite{ainleval2025}. This suggests simple features could help, but our TF-IDF baseline shows they are not enough.

\section{Experiments}

\subsection{Metrics}
Primary metric: \textbf{Accuracy} (as per competition rules).

We also report precision, recall, and F1-score per class, and visualize results with confusion matrices.

\subsection{Experiment Setup}
\begin{itemize}
    \item GPU: NVIDIA A100 40GB
    \item Precision: bfloat16
    \item Batch: 16
    \item Learning rate: 3e-5
    \item Epochs: 10, Early stopping: patience=3
    \item LoRA: r=8, alpha=16, targets: q\_proj, v\_proj
\end{itemize}

\subsection{Baselines}
\begin{itemize}
    \item TF-IDF + Logistic Regression
    \item ruBERT-tiny fine-tuned
    \item sastsy~\cite{ainleval2025}: 1st place winner (GigaCheck-based)
\end{itemize}

\section{Results}

Tab.~\ref{tab:results} shows our main results. The "Base" column is accuracy without any unknown detection -- the model predicts one of 4 known classes for every sample. "Confidence" uses softmax threshold: if max probability is below a threshold, predict "unknown". "Mahalanobis" uses distance in embedding space.

\begin{table}[tbh!]
\centering
\begin{tabular}{|l|ccc|}
\hline
Method & Base & Confidence & Mahalanobis \\
\hline
\multicolumn{4}{|l|}{\textit{AINL-Eval 2025 results~\cite{ainleval2025}:}} \\
TF-IDF baseline (competition) & 80.81\% & -- & -- \\
sastsy (1st place)~\cite{ainleval2025} & 91.22\% & -- & -- \\
\hline
\multicolumn{4}{|l|}{\textit{Our experiments:}} \\
TF-IDF + LogReg & 76.85\% & 81.06\% & -- \\
ruBERT-tiny (fine-tuned) & 78.29\% & 80.29\% & 85.25\% \\
Qwen2.5 + Dual-Head (ours) & 79.53\% & 82.61\% & \textbf{89.97\%} \\
\hline
\end{tabular}
\caption{Comparison of methods with different OOD detection strategies.}
\label{tab:results}
\end{table}

\textbf{Why does Mahalanobis work so much better?} Softmax confidence measures how "sure" the model is about its prediction, but a model trained only on GPT-4/Llama/Gemma will confidently assign GigaChat samples to one of these classes -- it has no concept of "none of the above." Mahalanobis distance, computed on the shared layer embeddings, measures geometric distance to class clusters. Unknown AI models produce embeddings that are far from all known class centroids, making them detectable.

\textbf{Key findings}:
\begin{enumerate}
    \item Mahalanobis boosts accuracy from 79.53\% to 89.97\% (+10.4\%). The binary head alone achieves 95.38\% (human vs AI is easy), and unknown recall reaches 76.25\%.
    \item ruBERT-tiny2 with Mahalanobis achieves 85.25\% -- only 4.7\% below Qwen, but 20x faster and 127x smaller. This makes real-time CPU deployment feasible.
\end{enumerate}

\begin{figure}[!tbh]
\centering
\includegraphics[width=0.8\linewidth]{mahalanobis_comparison.png}
\caption{OOD detection methods comparison. Mahalanobis distance achieves the best accuracy (89.97\%) on the dev set.}
\label{fig:comparison}
\end{figure}

\subsection{Knowledge Distillation Results}

We distill Qwen to ruBERT-tiny using DisRanker approach~\cite{disranker2024}:
$$\mathcal{L} = \alpha \cdot \mathcal{L}_{KL}(p_s, p_t) \cdot T^2 + (1-\alpha) \cdot \mathcal{L}_{CE}(p_s, y)$$
where $T=4$ (temperature), $\alpha=0.7$.

We compare two approaches:
\begin{itemize}
    \item Fresh BERT + KD: training from scratch with distillation
    \item Fine-tuned BERT + KD: further training of already fine-tuned model
\end{itemize}

\begin{table}[tbh!]
\centering
\begin{tabular}{|l|cccc|}
\hline
Model & Size & Inference & Raw Acc & +Mahalanobis \\
\hline
Qwen2.5-7B (teacher) & 15 GB & $\sim$300ms (GPU) & 79.53\% & \textbf{89.97\%} \\
ruBERT-tiny2 (fine-tuned) & 118 MB & $\sim$15ms (CPU) & 78.29\% & 85.25\% \\
Fresh BERT + KD & 118 MB & $\sim$15ms (CPU) & 74.21\% & 80.44\% \\
Fine-tuned BERT + KD & 118 MB & $\sim$15ms (CPU) & 77.01\% & 85.25\% \\
\hline
\end{tabular}
\caption{Teacher vs Student comparison. Qwen: 7.61B params~\cite{qwen2024}, ruBERT-tiny2: 29M params~\cite{ruberttiny2} (260$\times$ fewer params, 127$\times$ smaller file size). Inference times based on~\cite{bertcpu2022}.}
\label{tab:distill}
\end{table}

\textbf{Observation}: Distillation from fresh BERT achieves lower accuracy than fine-tuning baseline. This is expected since fresh BERT requires more training to learn from scratch. Fine-tuned BERT + KD achieves the same accuracy as the baseline, suggesting the model has already converged.

\section{Conclusion}

The main takeaway from this work: \textbf{detecting unknown AI models is hard, but Mahalanobis distance makes it tractable}. Standard confidence-based methods fail because neural networks are confidently wrong on out-of-distribution samples. Mahalanobis operates in embedding space where "unknown" means "far from everything I've seen" - a much more robust criterion.

Our best model (Qwen2.5-7B + Dual-Head + Mahalanobis) achieves 89.97\% accuracy, with 76.25\% recall on the unknown class. But perhaps more interesting is that ruBERT-tiny2 - a model 260x smaller - reaches 85.25\% with the same Mahalanobis approach. This suggests that the OOD detection method matters more than model size for this task.

\textbf{Practical implications}: A 29M parameter model running in 15ms on CPU can detect AI-generated scientific abstracts with reasonable accuracy. This enables deployment in resource-constrained environments - browser extensions, email filters, or mobile apps - without GPU infrastructure.

\textbf{Limitations}: Mahalanobis requires computing class statistics from training data embeddings upfront. If the distribution of AI models shifts (new models appear), these statistics need recomputation. Future work could explore online or adaptive OOD detection methods.

\bibliographystyle{apalike}
\begin{thebibliography}{9}

\bibitem{gigacheck2024}
Tolstykh, I., Tsybina, A., Yakubson, S., Gordeev, A., Dokholyan, V., Kuprashevich, M. (2024).
GigaCheck: Detecting LLM-generated Content.
\textit{arXiv preprint arXiv:2410.23728}.

\bibitem{qwen2024}
Qwen Team. (2024).
Qwen2 Technical Report.
\textit{arXiv preprint arXiv:2407.10671}.

\bibitem{hu2022lora}
Hu, E. J., et al. (2022).
LoRA: Low-Rank Adaptation of Large Language Models.
\textit{ICLR 2022}.

\bibitem{disranker2024}
Ye, D., et al. (2024).
Best Practices for Distilling Large Language Models into BERT for Web Search Ranking.
\textit{arXiv preprint arXiv:2411.04539}.

\bibitem{ainleval2025}
Batura, T., Bruches, E., Shvenk, M., Malykh, V. (2025).
AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian.
\textit{arXiv preprint arXiv:2508.09622}.
\url{https://codalab.lisn.upsaclay.fr/competitions/21895}

\bibitem{mahalanobis2018}
Lee, K., et al. (2018).
A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks.
\textit{NeurIPS 2018}.

\bibitem{ruberttiny2}
Dale, D. (2022).
ruBERT-tiny2: Russian Sentence Encoder.
\textit{Habr}.
\url{https://habr.com/ru/post/669674/},
\url{https://huggingface.co/cointegrated/rubert-tiny2}

\bibitem{bertcpu2022}
Boudier, M., Music, D. (2022).
Scaling up BERT-like model Inference on modern CPU.
\textit{Hugging Face Blog}.
\url{https://huggingface.co/blog/bert-cpu-scaling-part-1}

\end{thebibliography}

\end{document}
