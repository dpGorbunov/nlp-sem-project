\documentclass{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtext}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}

\title{Qwen2.5 Dual-Head for AI-Generated Text Detection:\\Reproducing AINL-Eval 2025 Winner with Knowledge Distillation}
\author{Dmitry Gorbunov\\ITMO University, AI Talent Hub}
\date{2025}

\begin{document}
\maketitle

\begin{abstract}
We reproduce the AINL-Eval 2025 winning solution (sastsy) for detecting AI-generated scientific abstracts in Russian. Our contributions: (1) Qwen2.5-7B backbone with Dual-Head architecture achieving 89.97\% dev accuracy; (2) Mahalanobis distance-based OOD detection for unknown class (+10.4\% improvement); (3) Practical finding: fine-tuned ruBERT-tiny2 (29M params) with Mahalanobis achieves 85.25\% accuracy with 20x faster inference (15ms CPU vs 300ms GPU) and 127x smaller model size (118MB vs 15GB). Code: \url{https://github.com/dpGorbunov/nlp-sem-project}.
\end{abstract}

\section{Introduction}

The proliferation of large language models (LLMs) poses threats to academic integrity. Detection of machine-generated text is crucial for scientific publications.

AINL-Eval 2025 shared task~\cite{ainleval2025} addresses this problem for Russian scientific abstracts with classification into 5 classes: human, GPT-4-turbo, Llama-3.3-70B, Gemma-2-27B, and unknown.

The winner sastsy~\cite{ainleval2025} achieved 91.22\% dev accuracy using GigaCheck~\cite{gigacheck2024} with Dual-Head modification. Our work reproduces this approach with improvements:

\begin{enumerate}
    \item \textbf{Qwen2.5-7B}: stronger backbone than Mistral-7B on benchmarks
    \item \textbf{Mahalanobis OOD Detection}: distance-based method for unknown class detection (+10\% over baseline)
    \item \textbf{Knowledge Distillation}: distillation to ruBERT-tiny for CPU inference
\end{enumerate}

\subsection{Team}
\textbf{Dmitry Gorbunov} -- model architecture design, experiments, report writing.

\section{Related Work}
\label{sec:related}

\subsection{GigaCheck}
GigaCheck~\cite{gigacheck2024} is a framework for LLM-generated content detection:
\begin{itemize}
    \item Backbone: Mistral-7B with LoRA (r=8, alpha=16)
    \item Pooling: EOS token only
    \item Classification: single head with CrossEntropy loss
\end{itemize}

\subsection{sastsy Solution}
The winning team sastsy~\cite{ainleval2025} extended GigaCheck with Dual-Head architecture:
\begin{itemize}
    \item Binary Head: human (0) vs AI (1)
    \item Multiclass Head: GPT-4, Llama, Gemma, Unknown
\end{itemize}

\subsection{LoRA}
Low-Rank Adaptation~\cite{hu2022lora} enables efficient LLM fine-tuning:
$$W' = W + \frac{\alpha}{r} \cdot BA$$
where $r$ is rank, $\alpha$ is scaling factor.

\subsection{Knowledge Distillation}
DisRanker~\cite{disranker2024} demonstrates effective LLM-to-BERT distillation with 10x speedup and minimal quality loss.

\subsection{Mahalanobis Distance for OOD Detection}
Mahalanobis distance~\cite{mahalanobis2018} is a powerful method for out-of-distribution (OOD) detection. For a sample embedding $x$, the distance to class $c$ is:
$$D_M(x, c) = \sqrt{(x - \mu_c)^T \Sigma^{-1} (x - \mu_c)}$$
where $\mu_c$ is the class mean and $\Sigma$ is the tied covariance matrix. Samples with high minimum distance across all classes are classified as OOD (unknown).

\section{Model Description}

\subsection{Architecture Overview}
Following sastsy~\cite{ainleval2025}, our architecture consists of:
\begin{enumerate}
    \item Backbone: Qwen2.5-7B + LoRA (r=8, alpha=16)
    \item Pooling: EOS token (last token with left padding)
    \item Shared Layer: 2-layer MLP with tanh activation
    \item Dual-Head: Binary + Multiclass classification
\end{enumerate}

The key difference is the backbone: we use Qwen2.5-7B instead of Mistral-7B.

\subsection{Dual-Head Architecture}
Following sastsy~\cite{ainleval2025}:
\begin{itemize}
    \item Binary Head: human (0) vs AI (1)
    \item Multiclass Head: GPT-4, Llama, Gemma, Unknown
\end{itemize}

Loss function:
$$\mathcal{L} = \mathcal{L}_{CE}^{bin} + \mathcal{L}_{CE}^{multi}$$
where multiclass loss ignores human samples (\texttt{ignore\_index=-1}).

Inference: if binary=0 $\rightarrow$ human, else use multiclass prediction.

\subsection{Why Qwen2.5 over Mistral}
According to Qwen2 Technical Report~\cite{qwen2024}, Qwen2-7B outperforms Mistral-7B on standard benchmarks (Tab.~\ref{tab:qwen_mistral}).

\begin{table}[tbh!]
\centering
\begin{tabular}{|l|ccc|}
\hline
Benchmark & Qwen2-7B & Mistral-7B & $\Delta$ \\
\hline
MMLU & \textbf{70.3} & 64.2 & +6.1 \\
HumanEval & \textbf{51.2} & 29.3 & +21.9 \\
GSM8K & \textbf{79.9} & 52.2 & +27.7 \\
\hline
\end{tabular}
\caption{Qwen2-7B vs Mistral-7B benchmark comparison.}
\label{tab:qwen_mistral}
\end{table}

\section{Dataset}

We use the AINL-Eval 2025 dataset~\cite{ainleval2025} for AI-generated Russian scientific abstract detection.

\begin{table}[tbh!]
\centering
\begin{tabular}{|l|ccc|}
\hline
 & Train & Dev & Test \\
\hline
Samples & 35,158 & 10,978 & 6,169 \\
Classes & 4 & 5 & 5 \\
\hline
\end{tabular}
\caption{AINL-Eval 2025 dataset statistics.}
\label{tab:dataset}
\end{table}

\textbf{Train classes}: human, GPT-4-Turbo, Llama-3.3-70B, Gemma-2-27B.

\textbf{Unknown class}: GigaChat-Lite in dev, DeepSeek-V3 in test (unseen during training).

\textbf{Key observation}: Human texts average 126 words vs 50--86 for AI models, and contain 10x more digits~\cite{ainleval2025}.

\textbf{OOD detection challenge}: The unknown class is absent from training data. We compare softmax confidence threshold and Mahalanobis distance~\cite{mahalanobis2018}.

\section{Experiments}

\subsection{Metrics}
Primary metric: \textbf{Accuracy} (as per competition rules).

We also report precision, recall, and F1-score per class, and visualize results with confusion matrices.

\subsection{Experiment Setup}
\begin{itemize}
    \item GPU: NVIDIA A100 40GB
    \item Precision: bfloat16
    \item Batch: 16
    \item Learning rate: 3e-5
    \item Epochs: 10, Early stopping: patience=3
    \item LoRA: r=8, alpha=16, targets: q\_proj, v\_proj
\end{itemize}

\subsection{Baselines}
\begin{itemize}
    \item TF-IDF + Logistic Regression
    \item ruBERT-tiny fine-tuned
    \item sastsy (GigaCheck)~\cite{gigacheck2024}: 1st place winner
\end{itemize}

\section{Results}

Results are presented in Tab.~\ref{tab:results}. We compare two OOD detection methods: softmax confidence threshold and Mahalanobis distance.

\begin{table}[tbh!]
\centering
\begin{tabular}{|l|ccc|}
\hline
Method & Base & Confidence & Mahalanobis \\
\hline
\multicolumn{4}{|l|}{\textit{AINL-Eval 2025 results~\cite{ainleval2025}:}} \\
TF-IDF baseline (competition) & 80.81\% & -- & -- \\
sastsy (1st place)~\cite{ainleval2025} & 91.22\% & -- & -- \\
\hline
\multicolumn{4}{|l|}{\textit{Our experiments:}} \\
TF-IDF + LogReg & -- & 82.62\% & -- \\
ruBERT-tiny (fine-tuned) & 78.29\% & 85.25\% & 85.25\% \\
Qwen2.5 + Dual-Head (ours) & 79.53\% & 82.61\% & \textbf{89.97\%} \\
\hline
\end{tabular}
\caption{Comparison of methods with different OOD detection strategies.}
\label{tab:results}
\end{table}

\textbf{Key findings}:
\begin{enumerate}
    \item Mahalanobis distance significantly improves unknown class detection, boosting Qwen accuracy from 79.53\% to 89.97\% (+10.4\%). The binary head achieves 95.38\% accuracy, unknown recall reaches 76.25\%.
    \item \textbf{Practical deployment}: ruBERT-tiny2 (118MB, 29M params) with Mahalanobis achieves 85.25\% accuracy — only 4.7\% below Qwen SOTA, but with \textbf{20x faster inference} ($\sim$15ms CPU vs $\sim$300ms GPU) and \textbf{127x smaller} model size. This enables real-time CPU deployment without GPU.
\end{enumerate}

\begin{figure}[!tbh]
\centering
\includegraphics[width=0.8\linewidth]{mahalanobis_comparison.png}
\caption{OOD detection methods comparison. Mahalanobis distance achieves the best accuracy (89.97\%) on the dev set.}
\label{fig:comparison}
\end{figure}

\subsection{Knowledge Distillation Results}

We distill Qwen to ruBERT-tiny using DisRanker approach~\cite{disranker2024}:
$$\mathcal{L} = \alpha \cdot \mathcal{L}_{KL}(p_s, p_t) \cdot T^2 + (1-\alpha) \cdot \mathcal{L}_{CE}(p_s, y)$$
where $T=4$ (temperature), $\alpha=0.7$.

We compare two approaches:
\begin{itemize}
    \item Fresh BERT + KD: training from scratch with distillation
    \item Fine-tuned BERT + KD: further training of already fine-tuned model
\end{itemize}

\begin{table}[tbh!]
\centering
\begin{tabular}{|l|cccc|}
\hline
Model & Size & Inference & Raw Acc & +Mahalanobis \\
\hline
Qwen2.5-7B (teacher) & 15 GB & $\sim$300ms (GPU) & 79.53\% & \textbf{89.97\%} \\
ruBERT-tiny2 (fine-tuned) & 118 MB & $\sim$15ms (CPU) & 78.29\% & 85.25\% \\
Fresh BERT + KD & 118 MB & $\sim$15ms (CPU) & 74.21\% & 80.44\% \\
Fine-tuned BERT + KD & 118 MB & $\sim$15ms (CPU) & 77.01\% & 85.25\% \\
\hline
\end{tabular}
\caption{Teacher vs Student comparison. Qwen: 7.61B params~\cite{qwen2024}, ruBERT-tiny2: 29M params~\cite{ruberttiny2} (260$\times$ smaller). Inference times based on~\cite{bertcpu2022}.}
\label{tab:distill}
\end{table}

\textbf{Observation}: Distillation from fresh BERT achieves lower accuracy than fine-tuning baseline. This is expected since fresh BERT requires more training to learn from scratch. Fine-tuned BERT + KD achieves the same accuracy as the baseline, suggesting the model has already converged.

\section{Conclusion}

We reproduced the sastsy (AINL-Eval 2025 winner) approach for AI-generated text detection with the following contributions:

\begin{enumerate}
    \item \textbf{Qwen2.5-7B backbone}: replacing Mistral-7B with Qwen2.5-7B-Instruct
    \item \textbf{Mahalanobis OOD detection}: distance-based method for unknown class, improving accuracy from 79.53\% to 89.97\% (+10.4\%)
    \item \textbf{Practical deployment}: ruBERT-tiny2 achieves 85.25\% with Mahalanobis (20x faster, 127x smaller than Qwen)
\end{enumerate}

\textbf{Summary of results}:
\begin{itemize}
    \item Best model: Qwen2.5 + Dual-Head + Mahalanobis = \textbf{89.97\%} dev accuracy
    \item Binary classification (human vs AI): 95.38\%
    \item Unknown class recall: 76.25\%
\end{itemize}

\textbf{Key practical finding}: Fine-tuned ruBERT-tiny2 (118MB, 29M params) with Mahalanobis OOD detection achieves \textbf{85.25\%} accuracy — only 4.7\% below the best Qwen model, but with \textbf{20x faster inference} ($\sim$15ms on CPU vs $\sim$300ms on GPU) and \textbf{127x smaller} model size (118MB vs 15GB). This demonstrates that lightweight models combined with proper OOD detection can approach LLM-level performance while enabling real-time CPU deployment without specialized hardware.

\textbf{Limitations}: Mahalanobis requires pre-computing class statistics from training embeddings. Future work could explore online estimation or more efficient OOD methods.

\bibliographystyle{apalike}
\begin{thebibliography}{9}

\bibitem{gigacheck2024}
Tolstykh, I., Tsybina, A., Yakubson, S., Gordeev, A., Dokholyan, V., Kuprashevich, M. (2024).
GigaCheck: Detecting LLM-generated Content.
\textit{arXiv preprint arXiv:2410.23728}.

\bibitem{qwen2024}
Qwen Team. (2024).
Qwen2 Technical Report.
\textit{arXiv preprint arXiv:2407.10671}.

\bibitem{hu2022lora}
Hu, E. J., et al. (2022).
LoRA: Low-Rank Adaptation of Large Language Models.
\textit{ICLR 2022}.

\bibitem{disranker2024}
Zhang, Y., et al. (2024).
Best Practices for Distilling Large Language Models into BERT for Web Search Ranking.
\textit{arXiv preprint arXiv:2411.04539}.

\bibitem{ainleval2025}
AINL-Eval 2025 Organizers. (2025).
AINL-Eval 2025 Shared Task: AI-Generated Scientific Abstract Detection.
\textit{arXiv preprint arXiv:2508.09622}.
\url{https://codalab.lisn.upsaclay.fr/competitions/21895}

\bibitem{mahalanobis2018}
Lee, K., et al. (2018).
A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks.
\textit{NeurIPS 2018}.

\bibitem{ruberttiny2}
Kolesnikova, A. (2022).
Knowledge Distillation of Russian Language Models with Reduction of Vocabulary.
\textit{arXiv preprint arXiv:2205.02340}.
\url{https://huggingface.co/cointegrated/rubert-tiny2}

\bibitem{bertcpu2022}
Boudier, M., Music, D. (2022).
Scaling up BERT-like model Inference on modern CPU.
\textit{Hugging Face Blog}.
\url{https://huggingface.co/blog/bert-cpu-scaling-part-1}

\end{thebibliography}

\end{document}
